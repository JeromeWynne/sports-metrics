{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "# code to retrieve player links from epl website (seasons 0809 - 1617)\n",
    "\n",
    "# urlopen wasn't working because the ascii codec that urllib uses to interpret urls didn't understand\n",
    "# non-ascii characters (such as accented letters). To solve this problem, I used urllib's quote() function,\n",
    "# which replaces special characters in the string with the hex %HH which is understood by ascii, and will address\n",
    "# the correct webpage. The 'safe' characters indicate which characters shouldn't be encoded in the way described.\n",
    "# It was necessary to encode the url to utf-8 beforehand because Python's default string format is Unicode.\n",
    "\n",
    "# we have the htmls for seasons 0809 - 1617\n",
    "player_lists_filepaths = ['./data/epl/epl_{:0>2}{:0>2}_players.html'.format(i, i+1) for i in range(14, 17) ]\n",
    "overview_urls = []\n",
    "for fp in player_lists_filepaths:\n",
    "    f = open(fp, encoding='utf-8')\n",
    "    players_page = BeautifulSoup(f.read(), 'html.parser')\n",
    "    player_tags = players_page.findAll('a', {'class':'playerName'})\n",
    "    more_urls = ['https:'+tag['href'] for tag in player_tags]\n",
    "    overview_urls = overview_urls + more_urls\n",
    "\n",
    "overview_urls = list(set(overview_urls))\n",
    "players = {}\n",
    "j = 0\n",
    "\n",
    "for url in overview_urls:\n",
    "        j += 1\n",
    "        player_info = {}\n",
    "        f_overview = urllib.request.urlopen(urllib.parse.quote(url, safe=':/', encoding='utf-8')).read()\n",
    "        f_stats = urllib.request.urlopen(urllib.parse.quote(url.replace('overview','stats'), safe=':/', encoding='utf-8')).read()\n",
    "        stats_page = BeautifulSoup(f_stats, 'html.parser')\n",
    "        overview_page = BeautifulSoup(f_overview, 'html.parser')\n",
    "        try:\n",
    "            name = stats_page.find('div', {'class':'name'}).string\n",
    "            stats_info = stats_page.findAll('span', {'class':'allStatContainer'})\n",
    "            overview_info = overview_page.find('div', {'class':'personalLists'}).findAll('li')\n",
    "            for stat in stats_info:\n",
    "                try:\n",
    "                    stat_value = stat.text.split()[0]\n",
    "                    player_info[stat['class'][1].replace('stat', '')] = stat_value\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "            for li in overview_info:\n",
    "                label = li.find('div', {'class':'label'}).string\n",
    "                try: # for nationality\n",
    "                    nationality = li.find('span', {'class':'playerCountry'}).string\n",
    "                    player_info['Nation'] = nationality\n",
    "                except AttributeError: # for player info that isn't nationality\n",
    "                    info_value = li.find('div', {'class':'info'}).string\n",
    "                    player_info[label] = info_value\n",
    "            player_name = overview_page.find('div', {'class':'label'}, text='Position').next_sibling.next_sibling.string\n",
    "            player_info['Position'] = player_name\n",
    "            players[name] = player_info\n",
    "        except AttributeError: # some players don't have a page :o\n",
    "            pass\n",
    "        if j % 100 == 0:\n",
    "            print(j)\n",
    "            try:\n",
    "                prev_data = pd.read_csv('./data/epl/players.csv', index_col=0)\n",
    "            except OSError:\n",
    "                prev_data = pd.DataFrame()\n",
    "            current_data = pd.DataFrame(players).transpose()\n",
    "            all_data = pd.concat([current_data, prev_data])\n",
    "            all_data.to_csv('./data/epl/players.csv', encoding='utf-8')\n",
    "            players = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = overview_urls[2400]\n",
    "player_info = {}\n",
    "f_overview = urllib.request.urlopen(urllib.parse.quote(url, safe=':/', encoding='utf-8')).read()\n",
    "f_stats = urllib.request.urlopen(urllib.parse.quote(url.replace('overview','stats'), safe=':/', encoding='utf-8')).read()\n",
    "stats_page = BeautifulSoup(f_stats, 'html.parser')\n",
    "overview_page = BeautifulSoup(f_overview, 'html.parser')\n",
    "try:\n",
    "    name = stats_page.find('div', {'class':'name'}).string\n",
    "    stats_info = stats_page.findAll('span', {'class':'allStatContainer'})\n",
    "    overview_info = overview_page.find('div', {'class':'personalLists'}).findAll('li')\n",
    "    for stat in stats_info:\n",
    "        try:\n",
    "            stat_value = stat.text.split()[0]\n",
    "            player_info[stat['class'][1].replace('stat', '')] = stat_value\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    for li in overview_info:\n",
    "        label = li.find('div', {'class':'label'}).string\n",
    "        try: # for nationality\n",
    "            nationality = li.find('span', {'class':'playerCountry'}).string\n",
    "            player_info['Nation'] = nationality\n",
    "        except AttributeError: # for player info that isn't nationality\n",
    "            info_value = li.find('div', {'class':'info'}).string\n",
    "            player_info[label] = info_value\n",
    "    player_name = overview_page.find('div', {'class':'label'}, text='Position').next_sibling.next_sibling.string\n",
    "    player_info['Position'] = player_name\n",
    "    players[name] = player_info\n",
    "except AttributeError: # some players don't have a page :o\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code to retrieve all players information on EPL website (takes about 5 hours to run!)\n",
    "\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "fp = 'https://www.premierleague.com/players/'\n",
    "players = {}\n",
    "player_strainer = SoupStrainer('div', {'class':'name'})\n",
    "stat_strainer = SoupStrainer('span', {'class':'stat'})\n",
    "\n",
    "for i in range(1, 100):\n",
    "    old_page = BeautifulSoup(urllib.request.urlopen(fp+str(i)).read(), 'lxml', parse_only=player_strainer)\n",
    "    try:\n",
    "        player_name = old_page.text\n",
    "        player_fp = fp+str(i)+'/'+player_name.replace(' ', '-')+'/stats'\n",
    "        stats_page = BeautifulSoup(urllib.request.urlopen(player_fp).read(), 'lxml', parse_only=stat_strainer)\n",
    "        all_stats = stats_page.findAll('span', {'class':'stat'})\n",
    "        player_stats = {}\n",
    "        for stat in all_stats:\n",
    "            try:\n",
    "                stat_value = re.findall('[-+]?\\d*\\.\\d+|\\d+', stat.span.string)[0]\n",
    "                player_stats[stat.span['class'][1].replace('stat','')] = float(stat_value)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        players[player_name] = player_stats\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "elapsed = timeit.default_timer() - start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
